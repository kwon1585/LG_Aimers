{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "273b75f6-25fa-4f9c-8c9d-fa33a985f2dd",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c415d9d-1111-46ed-bc2d-849c75ab962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f9c1c-39bc-42e7-a00d-3062d11b4f1e",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee76c413-001b-475e-9d1c-6662d25d2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "train_df = train_df.drop(columns=['PRODUCT_ID', 'TIMESTAMP', 'Y_Class', 'PRODUCT_CODE'])\n",
    "test_line = test_df['LINE']\n",
    "test_df = test_df.drop(columns=['PRODUCT_ID', 'TIMESTAMP', 'LINE', 'PRODUCT_CODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e05cac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.fillna('NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885f1e6b-97f4-4f90-b3ce-0513b7196db6",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c72765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in train_df.iterrows():\n",
    "  if data['LINE'] == 'T010305':\n",
    "    idx1 = idx\n",
    "    break\n",
    "data_line_1 = pd.DataFrame(train_df.loc[idx]).transpose()\n",
    "train_df = train_df.drop(idx)\n",
    "\n",
    "for idx, data in train_df.iterrows():\n",
    "  if data['LINE'] == 'T010306':\n",
    "    idx2 = idx\n",
    "    break\n",
    "data_line_2 = pd.DataFrame(train_df.loc[idx]).transpose()\n",
    "train_df = train_df.drop(idx)\n",
    "\n",
    "for idx, data in train_df.iterrows():\n",
    "  if data['LINE'] == 'T050304':\n",
    "    idx3 = idx\n",
    "    break\n",
    "data_line_3 = pd.DataFrame(train_df.loc[idx]).transpose()\n",
    "train_df = train_df.drop(idx)\n",
    "\n",
    "for idx, data in train_df.iterrows():\n",
    "  if data['LINE'] == 'T050307':\n",
    "    idx4 = idx\n",
    "    break\n",
    "data_line_4 = pd.DataFrame(train_df.loc[idx]).transpose()\n",
    "train_df = train_df.drop(idx)\n",
    "\n",
    "for idx, data in train_df.iterrows():\n",
    "  if data['LINE'] == 'T100304':\n",
    "    idx5 = idx\n",
    "    break\n",
    "data_line_5 = pd.DataFrame(train_df.loc[idx]).transpose()\n",
    "train_df = train_df.drop(idx)\n",
    "\n",
    "for idx, data in train_df.iterrows():\n",
    "  if data['LINE'] == 'T100306':\n",
    "    idx6 = idx\n",
    "    break\n",
    "data_line_6 = pd.DataFrame(train_df.loc[idx]).transpose()\n",
    "train_df = train_df.drop(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b73b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in train_df.iterrows():\n",
    "  if data['LINE'] == 'T010305': \n",
    "    data_tmp = pd.DataFrame(data).transpose()\n",
    "    data_line_1 = pd.concat([data_line_1, data_tmp])\n",
    "  elif data['LINE'] == 'T010306': \n",
    "    data_tmp = pd.DataFrame(data).transpose()\n",
    "    data_line_2 = pd.concat([data_line_2, data_tmp])\n",
    "  elif data['LINE'] == 'T050304': \n",
    "    data_tmp = pd.DataFrame(data).transpose()\n",
    "    data_line_3 = pd.concat([data_line_3, data_tmp])\n",
    "  elif data['LINE'] == 'T050307':   \n",
    "    data_tmp = pd.DataFrame(data).transpose()\n",
    "    data_line_4 = pd.concat([data_line_4, data_tmp])\n",
    "  elif data['LINE'] == 'T100304': \n",
    "    data_tmp = pd.DataFrame(data).transpose()\n",
    "    data_line_5 = pd.concat([data_line_5, data_tmp])\n",
    "  else :\n",
    "    data_tmp = pd.DataFrame(data).transpose()\n",
    "    data_line_6 = pd.concat([data_line_6, data_tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42fcd1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_line_1 = data_line_1.dropna(axis=1)\n",
    "data_line_2 = data_line_2.dropna(axis=1)\n",
    "data_line_3 = data_line_3.dropna(axis=1)\n",
    "data_line_4 = data_line_4.dropna(axis=1)\n",
    "data_line_5 = data_line_5.dropna(axis=1)\n",
    "data_line_6 = data_line_6.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fa9fb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5j/5y70dmpj6gz7vm0lv4mwhnzc0000gn/T/ipykernel_72717/1391748385.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  data_std.append(data_line_1.std())\n",
      "/var/folders/5j/5y70dmpj6gz7vm0lv4mwhnzc0000gn/T/ipykernel_72717/1391748385.py:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  data_std.append(data_line_2.std())\n",
      "/var/folders/5j/5y70dmpj6gz7vm0lv4mwhnzc0000gn/T/ipykernel_72717/1391748385.py:4: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  data_std.append(data_line_3.std())\n",
      "/var/folders/5j/5y70dmpj6gz7vm0lv4mwhnzc0000gn/T/ipykernel_72717/1391748385.py:5: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  data_std.append(data_line_4.std())\n",
      "/var/folders/5j/5y70dmpj6gz7vm0lv4mwhnzc0000gn/T/ipykernel_72717/1391748385.py:6: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  data_std.append(data_line_5.std())\n",
      "/var/folders/5j/5y70dmpj6gz7vm0lv4mwhnzc0000gn/T/ipykernel_72717/1391748385.py:7: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  data_std.append(data_line_6.std())\n"
     ]
    }
   ],
   "source": [
    "data_std = []\n",
    "data_std.append(data_line_1.std())\n",
    "data_std.append(data_line_2.std())\n",
    "data_std.append(data_line_3.std())\n",
    "data_std.append(data_line_4.std())\n",
    "data_std.append(data_line_5.std())\n",
    "data_std.append(data_line_6.std())\n",
    "\n",
    "for idx, x in data_std[0].iteritems():\n",
    "  if x == 0 or x >= 10000: del data_line_1[idx]\n",
    "for idx, x in data_std[1].iteritems():\n",
    "  if x == 0 or x >= 10000: del data_line_2[idx]\n",
    "for idx, x in data_std[2].iteritems():\n",
    "  if x == 0 or x >= 10000: del data_line_3[idx]\n",
    "for idx, x in data_std[3].iteritems():\n",
    "  if x == 0 or x >= 10000 : del data_line_4[idx]\n",
    "for idx, x in data_std[4].iteritems():\n",
    "  if x == 0 or x >= 10000: del data_line_5[idx]\n",
    "for idx, x in data_std[5].iteritems():\n",
    "  if x == 0 or x >= 10000: del data_line_6[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c15c708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index_1 = []\n",
    "data_index_2 = []\n",
    "data_index_3 = []\n",
    "data_index_4 = []\n",
    "data_index_5 = []\n",
    "data_index_6 = []\n",
    "\n",
    "for idx, data in data_line_1.iloc[0].iteritems():\n",
    "  if idx[0] == 'X': data_index_1.append(idx)\n",
    "\n",
    "for idx, data in data_line_2.iloc[0].iteritems():\n",
    "  if idx[0] == 'X': data_index_2.append(idx)\n",
    "\n",
    "for idx, data in data_line_3.iloc[0].iteritems():\n",
    "  if idx[0] == 'X': data_index_3.append(idx)\n",
    "\n",
    "for idx, data in data_line_4.iloc[0].iteritems():\n",
    "  if idx[0] == 'X': data_index_4.append(idx)\n",
    "\n",
    "for idx, data in data_line_5.iloc[0].iteritems():\n",
    "  if idx[0] == 'X': data_index_5.append(idx)\n",
    "\n",
    "for idx, data in data_line_6.iloc[0].iteritems():\n",
    "  if idx[0] == 'X': data_index_6.append(idx)\n",
    "\n",
    "data_index = []\n",
    "data_index.append(data_index_1)\n",
    "data_index.append(data_index_2)\n",
    "data_index.append(data_index_3)\n",
    "data_index.append(data_index_4)\n",
    "data_index.append(data_index_5)\n",
    "data_index.append(data_index_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26170d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_target_1 = data_line_1['Y_Quality'].values.tolist()\n",
    "data_target_2 = data_line_2['Y_Quality'].values.tolist()\n",
    "data_target_3 = data_line_3['Y_Quality'].values.tolist()\n",
    "data_target_4 = data_line_4['Y_Quality'].values.tolist()\n",
    "data_target_5 = data_line_5['Y_Quality'].values.tolist()\n",
    "data_target_6 = data_line_6['Y_Quality'].values.tolist()\n",
    "\n",
    "data_line_1 = data_line_1.drop(columns=['Y_Quality', 'LINE'])\n",
    "data_line_2 = data_line_2.drop(columns=['Y_Quality', 'LINE'])\n",
    "data_line_3 = data_line_3.drop(columns=['Y_Quality', 'LINE'])\n",
    "data_line_4 = data_line_4.drop(columns=['Y_Quality', 'LINE'])\n",
    "data_line_5 = data_line_5.drop(columns=['Y_Quality', 'LINE'])\n",
    "data_line_6 = data_line_6.drop(columns=['Y_Quality', 'LINE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4cef668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = []\n",
    "data_mean.append(data_line_1.mean())\n",
    "data_mean.append(data_line_2.mean())\n",
    "data_mean.append(data_line_3.mean())\n",
    "data_mean.append(data_line_4.mean())\n",
    "data_mean.append(data_line_5.mean())\n",
    "data_mean.append(data_line_6.mean())\n",
    "\n",
    "data_std = []\n",
    "data_std.append(data_line_1.std())\n",
    "data_std.append(data_line_2.std())\n",
    "data_std.append(data_line_3.std())\n",
    "data_std.append(data_line_4.std())\n",
    "data_std.append(data_line_5.std())\n",
    "data_std.append(data_line_6.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4cfbec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_line_1 = (data_line_1 - data_mean[0]) / data_std[0]\n",
    "data_line_2 = (data_line_2 - data_mean[1]) / data_std[1]\n",
    "data_line_3 = (data_line_3 - data_mean[2]) / data_std[2]\n",
    "data_line_4 = (data_line_4 - data_mean[3]) / data_std[3]\n",
    "data_line_5 = (data_line_5 - data_mean[4]) / data_std[4]\n",
    "data_line_6 = (data_line_6 - data_mean[5]) / data_std[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "677 680 736 585 167 533\n"
     ]
    }
   ],
   "source": [
    "len_line_1 = len(data_line_1.columns)\n",
    "len_line_2 = len(data_line_2.columns)\n",
    "len_line_3 = len(data_line_3.columns)\n",
    "len_line_4 = len(data_line_4.columns)\n",
    "len_line_5 = len(data_line_5.columns)\n",
    "len_line_6 = len(data_line_6.columns)\n",
    "\n",
    "print(len_line_1, len_line_2, len_line_3, len_line_4, len_line_5, len_line_6)\n",
    "\n",
    "data_line_1 = data_line_1.values.tolist()\n",
    "data_line_2 = data_line_2.values.tolist()\n",
    "data_line_3 = data_line_3.values.tolist()\n",
    "data_line_4 = data_line_4.values.tolist()\n",
    "data_line_5 = data_line_5.values.tolist()\n",
    "data_line_6 = data_line_6.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3574c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(lst):\n",
    "  for i in range(len(lst)):\n",
    "    lst[i] = [float(x) for x in lst[i]]\n",
    "  return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8ecd0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_line_1 = to_float(data_line_1)\n",
    "data_line_2 = to_float(data_line_2)\n",
    "data_line_3 = to_float(data_line_3)\n",
    "data_line_4 = to_float(data_line_4)\n",
    "data_line_5 = to_float(data_line_5)\n",
    "data_line_6 = to_float(data_line_6)\n",
    "\n",
    "data_target_1 = [float((x - 0.53089627)/0.00739057) for x in data_target_1]\n",
    "data_target_2 = [float((x - 0.53089627)/0.00739057) for x in data_target_2]\n",
    "data_target_3 = [float((x - 0.53089627)/0.00739057) for x in data_target_3]\n",
    "data_target_4 = [float((x - 0.53089627)/0.00739057) for x in data_target_4]\n",
    "data_target_5 = [float((x - 0.53089627)/0.00739057) for x in data_target_5]\n",
    "data_target_6 = [float((x - 0.53089627)/0.00739057) for x in data_target_6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "edd8c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_line_1 = torch.tensor(data_line_1)\n",
    "data_line_2 = torch.tensor(data_line_2)\n",
    "data_line_3 = torch.tensor(data_line_3)\n",
    "data_line_4 = torch.tensor(data_line_4)\n",
    "data_line_5 = torch.tensor(data_line_5)\n",
    "data_line_6 = torch.tensor(data_line_6)\n",
    "\n",
    "data_target_1 = torch.tensor(data_target_1)\n",
    "data_target_2 = torch.tensor(data_target_2)\n",
    "data_target_3 = torch.tensor(data_target_3)\n",
    "data_target_4 = torch.tensor(data_target_4)\n",
    "data_target_5 = torch.tensor(data_target_5)\n",
    "data_target_6 = torch.tensor(data_target_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec6b34-1e64-4d20-afdd-e96f4f77fa31",
   "metadata": {},
   "source": [
    "## Classification Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c6a395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Idea1(nn.Module):\n",
    "    def __init__(self, len_line):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(len_line, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6ca77bc-154c-4cb3-9251-3fe45d671416",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Idea1(len_line_1)\n",
    "model_2 = Idea1(len_line_2)\n",
    "model_3 = Idea1(len_line_3)\n",
    "model_4 = Idea1(len_line_4)\n",
    "model_5 = Idea1(len_line_5)\n",
    "model_6 = Idea1(len_line_6)\n",
    "\n",
    "optimizer_1 = torch.optim.Adam(model_1.parameters(), lr=1.0e-3)\n",
    "optimizer_2 = torch.optim.Adam(model_2.parameters(), lr=1.0e-3)\n",
    "optimizer_3 = torch.optim.Adam(model_3.parameters(), lr=1.0e-3)\n",
    "optimizer_4 = torch.optim.Adam(model_4.parameters(), lr=1.0e-3)\n",
    "optimizer_5 = torch.optim.Adam(model_5.parameters(), lr=1.0e-3)\n",
    "optimizer_6 = torch.optim.Adam(model_6.parameters(), lr=1.0e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fabdc15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5j/5y70dmpj6gz7vm0lv4mwhnzc0000gn/T/ipykernel_72717/1166497462.py:9: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(out, data_target_1[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0678983445694153\n",
      "0.039130401567721705\n",
      "0.08427539628486927\n",
      "0.03837018623368144\n",
      "0.05880513886896258\n",
      "0.03763404324393578\n",
      "0.049833347867491794\n",
      "0.0339930699795378\n",
      "0.07965773739916135\n",
      "0.030165613698388476\n",
      "0.03938629685944761\n",
      "0.03620899503831979\n",
      "0.05161317571582913\n",
      "0.03471263126047527\n",
      "0.03528839622012422\n",
      "0.025930393523933965\n",
      "0.037397612554918774\n",
      "0.03143910068344775\n",
      "0.06070120833484938\n",
      "0.03154026749186638\n",
      "0.062092578439795314\n",
      "0.030832816312654908\n",
      "0.058151793274854584\n",
      "0.03186117847285006\n",
      "0.05166916505225834\n",
      "0.027909020869043098\n",
      "0.050937605600326716\n",
      "0.030356009910716332\n",
      "0.046918349578837955\n",
      "0.022249933458100467\n",
      "0.03408676390458761\n",
      "0.027531551324680634\n",
      "0.027348661268799826\n",
      "0.023284392163560114\n",
      "0.06254760118324573\n",
      "0.05065861943037015\n",
      "0.07713840943986694\n",
      "0.05778016941961033\n",
      "0.0768031532772619\n",
      "0.07173997156422633\n",
      "0.13782134348334274\n",
      "0.03224964521581631\n",
      "0.05801878493189641\n",
      "0.01914657517928174\n",
      "0.012946378848173443\n",
      "0.005897419846486612\n",
      "0.007210885859488456\n",
      "0.004677213314909357\n",
      "0.004419510752421633\n",
      "0.0033734977535545383\n",
      "0.004127219409350785\n",
      "0.009070454052577542\n",
      "0.015759546971180035\n",
      "0.025199950912758017\n",
      "0.07331068166945057\n",
      "0.05856329875506852\n",
      "0.0973609780623396\n",
      "0.03758601013294661\n",
      "0.07946749493961669\n",
      "0.03078300519832275\n",
      "0.046318796106110526\n",
      "0.018435574995300026\n",
      "0.03170081583054479\n",
      "0.013563745094824437\n",
      "0.019835925831426657\n",
      "0.01922674456541023\n",
      "0.010048303381154255\n",
      "0.033743139860234805\n",
      "0.06810380892680784\n",
      "0.018783069487507402\n",
      "0.02811985433155792\n",
      "0.03075423026743303\n",
      "0.015648299468302308\n",
      "0.023754503784640946\n",
      "0.04434285477167524\n",
      "0.05269070568475222\n",
      "0.08964455417000508\n",
      "0.025498934243071328\n",
      "0.052085527696091295\n",
      "0.02862998403314906\n",
      "0.03789968567186899\n",
      "0.010895189427665163\n",
      "0.018520285110236914\n",
      "0.011843689869593036\n",
      "0.014208898721907861\n",
      "0.008564312293393892\n",
      "0.011281666999913518\n",
      "0.006130718854376073\n",
      "0.007361821317298755\n",
      "0.02057716152771487\n",
      "0.032458377459742054\n",
      "0.01425182556566034\n",
      "0.03695274595563236\n",
      "0.040475173118194596\n",
      "0.013913212964182376\n",
      "0.023972281686191005\n",
      "0.03294215131572239\n",
      "0.040735997129991215\n",
      "0.0639799338306084\n",
      "0.023460169761943626\n",
      "0.055411371044743196\n",
      "0.03195177646090127\n",
      "0.07360305611803142\n",
      "0.03747038327615922\n",
      "0.07774733579649133\n",
      "0.019664502917891496\n",
      "0.024253955813993203\n",
      "0.014773663897362201\n",
      "0.007341620552411167\n",
      "0.009488765401514129\n",
      "0.015852992695391872\n",
      "0.012332235681714399\n",
      "0.020713881936903124\n",
      "0.031774989815534414\n",
      "0.013251303109689739\n",
      "0.01869014865198871\n",
      "0.018246589259618657\n",
      "0.008681225885855853\n",
      "0.010860721724280296\n",
      "0.015208172779776432\n",
      "0.005958695566949561\n",
      "0.0043870416270384354\n",
      "0.004490659298726776\n",
      "0.0030203430003896917\n",
      "0.004743919970031569\n",
      "0.010252287500098661\n",
      "0.011607705461170492\n",
      "0.019736656072289683\n",
      "0.05021357890500902\n",
      "0.04753072409751678\n",
      "0.07546604245216243\n",
      "0.03481975767776622\n",
      "0.017301758531884533\n",
      "0.020832034866247094\n",
      "0.023332871885709903\n",
      "0.006302344217622981\n",
      "0.0044928989765347805\n",
      "0.004556346811499914\n",
      "0.003299336330609808\n",
      "0.00419598203987183\n",
      "0.007015335106214093\n",
      "0.01617415476197391\n",
      "0.01409275781231137\n",
      "0.01961289609883706\n",
      "0.006366327597365031\n",
      "0.0074730509804563335\n",
      "0.01082689919318863\n",
      "0.011670624075101563\n",
      "0.018150241924054095\n",
      "0.0333886738954132\n",
      "0.028755110852943893\n",
      "0.025892398747409028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5j/5y70dmpj6gz7vm0lv4mwhnzc0000gn/T/ipykernel_72717/1166497462.py:21: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(out, data_target_2[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010563052408558551\n",
      "0.013599507738637127\n",
      "0.012545190907491946\n",
      "0.01914428087457054\n",
      "0.022442482846466712\n",
      "0.030401784355066346\n",
      "0.022813821086570638\n",
      "0.045713727656870366\n",
      "0.03810939832716908\n",
      "0.06973286754458091\n",
      "0.05398565785485435\n",
      "0.09810335246511385\n",
      "0.047809032529845905\n",
      "0.06616706141989782\n",
      "0.06816384370977176\n",
      "0.1254774158326232\n",
      "0.06133198541995496\n",
      "0.0640628204764133\n",
      "0.015415380283575433\n",
      "0.008334822058422033\n",
      "0.00612425389990189\n",
      "0.007202424227866037\n",
      "0.006428280228513879\n",
      "0.007892203203471126\n",
      "0.005914620644996051\n",
      "0.011995815189018799\n",
      "0.010103015706643502\n",
      "0.024742453865298688\n",
      "0.053004132333356734\n",
      "0.07410992448131526\n",
      "0.07238108453839638\n",
      "0.1580221903488564\n",
      "0.12866805174063692\n",
      "0.17169862475254508\n",
      "0.22201715828923949\n",
      "0.1035485630173103\n",
      "0.04944738728473632\n",
      "0.05815276151302636\n",
      "0.019870285129986443\n",
      "0.005415586672080102\n",
      "0.0050543685475324665\n",
      "0.0041401294622931215\n",
      "0.0026610701860079696\n",
      "0.0022379221590672466\n",
      "0.0022711505077733056\n",
      "0.0021301711725941782\n",
      "0.002574371677362259\n",
      "0.003861630132107864\n",
      "0.005956468455877127\n",
      "0.006931664575246747\n",
      "0.01134099646151786\n",
      "0.009065974136037522\n",
      "0.018092027417804957\n",
      "0.016916662404880104\n",
      "0.03189805881732874\n",
      "0.03701217150087202\n",
      "0.020616946371813227\n",
      "0.028525373005624844\n",
      "0.06279949639544903\n",
      "0.062913442219646\n",
      "0.04567267187147601\n",
      "0.08388440357833003\n",
      "0.048032224055012715\n",
      "0.08215758234401196\n",
      "0.024889028088294855\n",
      "0.028793740702570046\n",
      "0.014296991094198169\n",
      "0.013263488839064459\n",
      "0.02029702426713941\n",
      "0.03767466282138149\n",
      "0.018054787938691114\n",
      "0.016130292649108924\n",
      "0.01929089021454534\n",
      "0.02090312224458444\n",
      "0.019629426190652208\n",
      "0.037556155587002746\n",
      "0.020276450724164428\n",
      "0.018347698508220236\n",
      "0.014918876794887735\n",
      "0.02601491913040458\n",
      "0.024306016036215235\n",
      "0.03725996166683306\n",
      "0.03792240244370727\n",
      "0.030029899495174466\n",
      "0.02827025973634006\n",
      "0.07276151131201562\n",
      "0.044527303044411125\n",
      "0.014266677066608021\n",
      "0.02221291087872877\n",
      "0.023251120767119864\n",
      "0.027587870660030538\n",
      "0.056336092163340776\n",
      "0.02960556021038688\n",
      "0.04320694343680316\n",
      "0.027027621110844433\n",
      "0.04076916619140515\n",
      "0.043254400158301516\n",
      "0.026392436437150276\n",
      "0.025801074696762172\n",
      "0.0360297874313395\n",
      "0.02660868540350282\n",
      "0.02371693604256474\n",
      "0.03745104312851107\n",
      "0.03917401951644674\n",
      "0.02182109614640077\n",
      "0.04731158351833074\n",
      "0.04278344312577766\n",
      "0.028280934701141243\n",
      "0.03205278247951437\n",
      "0.017399681640511387\n",
      "0.018199949811995405\n",
      "0.018314140888579848\n",
      "0.009847461949493663\n",
      "0.01017059831320028\n",
      "0.014533271603087476\n",
      "0.009568725445304998\n",
      "0.010254000880935824\n",
      "0.015049909475020534\n",
      "0.008097559342427627\n",
      "0.01639606224857094\n",
      "0.020331833240048322\n",
      "0.011532696723546516\n",
      "0.03334687103584884\n",
      "0.01485833112533328\n",
      "0.04547110553629276\n",
      "0.044830272212311686\n",
      "0.050087225599704\n",
      "0.09421817537065148\n",
      "0.038300358468741025\n",
      "0.054064904286058535\n",
      "0.05869067837275647\n",
      "0.03796540121145101\n",
      "0.02260224672669311\n",
      "0.03191414975279783\n",
      "0.009392060398704416\n",
      "0.010110407126675268\n",
      "0.007130039422574898\n",
      "0.002457937014228914\n",
      "0.004471593694257844\n",
      "0.003480240253272119\n",
      "0.0012333102435877358\n",
      "0.0038443675868190244\n",
      "0.0024120530248816457\n",
      "0.003438397799048484\n",
      "0.00628042247142925\n",
      "0.00632769067789606\n",
      "0.017499473538165538\n",
      "0.013229097753625345\n",
      "0.1826111887374614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5j/5y70dmpj6gz7vm0lv4mwhnzc0000gn/T/ipykernel_72717/1166497462.py:34: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(out, data_target_3[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12050092233428773\n",
      "0.2939599258083362\n",
      "0.09943816650896527\n",
      "0.20540406139497674\n",
      "0.06597601926067653\n",
      "0.10365685333398197\n",
      "0.06827878588136371\n",
      "0.10758025187128376\n",
      "0.07910227091078689\n",
      "0.05014092444346319\n",
      "0.03427014340366277\n",
      "0.024930551629643978\n",
      "0.03483148164901323\n",
      "0.03840948642581557\n",
      "0.029697701167839485\n",
      "0.035903462668428775\n",
      "0.029997955154694632\n",
      "0.02046506848397164\n",
      "0.03162810655033191\n",
      "0.0292126725342413\n",
      "0.018454965143973823\n",
      "0.03014728166079131\n",
      "0.031495622675985636\n",
      "0.031577127574004965\n",
      "0.04918942446085285\n",
      "0.0953220413235607\n",
      "0.06560071409088171\n",
      "0.09060949182083441\n",
      "0.07133031476203144\n",
      "0.10300753020173695\n",
      "0.033585617126846046\n",
      "0.055351269494132375\n",
      "0.041887652538003785\n",
      "0.041866523734717165\n",
      "0.021169210216813693\n",
      "0.018980225762089294\n",
      "0.02214684766054904\n",
      "0.024475244044318802\n",
      "0.01571711396115333\n",
      "0.01597062827149551\n",
      "0.020174698939242192\n",
      "0.03035933272200544\n",
      "0.024202154321973268\n",
      "0.04293962593423354\n",
      "0.032538585383311634\n",
      "0.051122336253024\n",
      "0.025905748324277784\n",
      "0.05594752405269569\n",
      "0.03839173975005139\n",
      "0.06278077216336733\n",
      "0.047962362918730295\n",
      "0.11132085887352161\n",
      "0.055606440856279686\n",
      "0.06727949886593566\n",
      "0.0411893185463412\n",
      "0.06000493974711564\n",
      "0.02774963692528532\n",
      "0.03946059171487366\n",
      "0.03595962424316085\n",
      "0.03134457558157395\n",
      "0.03274314140948357\n",
      "0.02856429854451429\n",
      "0.03016691593744233\n",
      "0.032021286219066254\n",
      "0.02881068316956095\n",
      "0.03446706834559932\n",
      "0.019377858065056968\n",
      "0.035536196109332265\n",
      "0.026895409768957716\n",
      "0.03187775067358883\n",
      "0.02982724198437399\n",
      "0.05067032522511466\n",
      "0.04326225505375101\n",
      "0.038831844457648076\n",
      "0.03091060312108869\n",
      "0.0528292574953206\n",
      "0.030197184302025084\n",
      "0.07045983219079642\n",
      "0.03190586679187536\n",
      "0.053405090779162306\n",
      "0.027307335866190934\n",
      "0.04340125647891609\n",
      "0.02502900614702715\n",
      "0.05296881781896445\n",
      "0.02790905148894405\n",
      "0.04876759102735844\n",
      "0.02702964251724383\n",
      "0.03148481508372469\n",
      "0.024092960644002057\n",
      "0.03706447952909578\n",
      "0.018457807121258624\n",
      "0.032900450026606\n",
      "0.028141084489792415\n",
      "0.0636647945421301\n",
      "0.04644499332282146\n",
      "0.05513891406820648\n",
      "0.03283466853961505\n",
      "0.050011435257287164\n",
      "0.030296044762377114\n",
      "0.05890135706068833\n",
      "0.04173393709842726\n",
      "0.05315682261699988\n",
      "0.0339319612228614\n",
      "0.032581386399984834\n",
      "0.02299967567110938\n",
      "0.019976247597773826\n",
      "0.020527365309578435\n",
      "0.016840037819986737\n",
      "0.013698526385759089\n",
      "0.013906590633612909\n",
      "0.01274878085926864\n",
      "0.017596075358284953\n",
      "0.01617662234365943\n",
      "0.036879700170511016\n",
      "0.027814131515394063\n",
      "0.04544844988450773\n",
      "0.04026180347257954\n",
      "0.043290806194375926\n",
      "0.02965421546572636\n",
      "0.039042205821610526\n",
      "0.027630117690306266\n",
      "0.020592237114226756\n",
      "0.019687315208278877\n",
      "0.024629373765917943\n",
      "0.018037008395321134\n",
      "0.02593389754323775\n",
      "0.020712885474419334\n",
      "0.023597924309814725\n",
      "0.028092685118976644\n",
      "0.01669380908463946\n",
      "0.020258464416119983\n",
      "0.02402131388930101\n",
      "0.014592122214780946\n",
      "0.02565840537670283\n",
      "0.01809340103325387\n",
      "0.02810059082199686\n",
      "0.0312872155563687\n",
      "0.03365435321337629\n",
      "0.03394205044877452\n",
      "0.0379999417957985\n",
      "0.022615827246868437\n",
      "0.0412443403742632\n",
      "0.0223906250189527\n",
      "0.02229123155739333\n",
      "0.01939349289610248\n",
      "0.030648321790324236\n",
      "0.027485142148859082\n",
      "0.03056356330918602\n",
      "0.01917526860846663\n",
      "0.024980362177703545\n",
      "0.04310188994735481\n",
      "0.06465007653479829\n",
      "0.11384089597112153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5j/5y70dmpj6gz7vm0lv4mwhnzc0000gn/T/ipykernel_72717/1166497462.py:47: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(out, data_target_4[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08099699865821836\n",
      "0.05894708510583174\n",
      "0.03463873405044502\n",
      "0.03285793312690138\n",
      "0.09619114279225219\n",
      "0.10829318362494805\n",
      "0.11693672034380143\n",
      "0.06270196052435446\n",
      "0.058556804813893644\n",
      "0.049645459305151816\n",
      "0.05972543766633213\n",
      "0.17402379681468533\n",
      "0.16221891543656447\n",
      "0.23716877258560826\n",
      "0.07350309306500658\n",
      "0.09800667972827103\n",
      "0.21647611574562256\n",
      "0.1400343410998522\n",
      "0.14548042528104435\n",
      "0.05031500303593497\n",
      "0.04546170265322352\n",
      "0.014586799641722877\n",
      "0.009417175173309391\n",
      "0.01273740293568099\n",
      "0.017230923224772033\n",
      "0.025105680877415926\n",
      "0.01950559394333333\n",
      "0.014186251228173725\n",
      "0.0067719298781114615\n",
      "0.008164361215709246\n",
      "0.02070316542695477\n",
      "0.027480306471813622\n",
      "0.029180959685141887\n",
      "0.016489947290113247\n",
      "0.016498892724219297\n",
      "0.05178904766691652\n",
      "0.07442756338666966\n",
      "0.09301600744539601\n",
      "0.04634193660517451\n",
      "0.03561699591996818\n",
      "0.07519651248748276\n",
      "0.0844594344213571\n",
      "0.12455407002804957\n",
      "0.06866138542370338\n",
      "0.0476515708794252\n",
      "0.04918668693369996\n",
      "0.0632664665657406\n",
      "0.08257167261035647\n",
      "0.14831939689013648\n",
      "0.16675892441846718\n",
      "0.13386807109154256\n",
      "0.1603994550321312\n",
      "0.34716108500184156\n",
      "0.12498757131778013\n",
      "0.05000434029794102\n",
      "0.02180498675299229\n",
      "0.0327170555728593\n",
      "0.018995847231616653\n",
      "0.013443739596642416\n",
      "0.026424350985572737\n",
      "0.03765191248666509\n",
      "0.04378151037351364\n",
      "0.02190821884375098\n",
      "0.015906998949620492\n",
      "0.009317951984764065\n",
      "0.012696872081559854\n",
      "0.02487368780311629\n",
      "0.02694662119771844\n",
      "0.027850137210320793\n",
      "0.012894283991754275\n",
      "0.008290923575405682\n",
      "0.009233326350137563\n",
      "0.015761800349146426\n",
      "0.029350344125613707\n",
      "0.028111222380238537\n",
      "0.024583101121866742\n",
      "0.011415047050642594\n",
      "0.011251424577992674\n",
      "0.029921179063034656\n",
      "0.04213979025455589\n",
      "0.05418546601330967\n",
      "0.03410948112207279\n",
      "0.024816068865645642\n",
      "0.02041524141675057\n",
      "0.031741279849863226\n",
      "0.0780909304516648\n",
      "0.08109197681132568\n",
      "0.08877877143684254\n",
      "0.058110416280296055\n",
      "0.07190588014516\n",
      "0.17916186655453376\n",
      "0.07926927851422945\n",
      "0.10528755875878103\n",
      "0.04113324802864597\n",
      "0.04279758018557394\n",
      "0.025796914809293844\n",
      "0.03347676445160427\n",
      "0.04526206065804131\n",
      "0.03240408126068706\n",
      "0.024309045006421434\n",
      "0.009842871860428204\n",
      "0.014434306044401117\n",
      "0.03858332828294224\n",
      "0.042976561214487674\n",
      "0.04351225894760362\n",
      "0.01616213548749372\n",
      "0.014368454551772845\n",
      "0.03498631596889503\n",
      "0.04859377665724467\n",
      "0.06503906248379854\n",
      "0.0302258641504509\n",
      "0.023139807483154244\n",
      "0.016938324072265504\n",
      "0.027182592586943395\n",
      "0.06001271144466392\n",
      "0.04884203306877615\n",
      "0.04510170991354184\n",
      "0.013560057838817773\n",
      "0.00916872330533568\n",
      "0.01896894864492707\n",
      "0.032521593137425545\n",
      "0.05346026967121817\n",
      "0.036559320864117104\n",
      "0.03124795885632704\n",
      "0.009782895141900857\n",
      "0.009793903298183674\n",
      "0.025873473334508207\n",
      "0.03344529104253438\n",
      "0.04068336102132332\n",
      "0.01762514280487505\n",
      "0.01202150187612547\n",
      "0.01653443610954734\n",
      "0.030647404605111832\n",
      "0.0555732670581208\n",
      "0.03921811535777427\n",
      "0.044821602243874145\n",
      "0.04348565495681777\n",
      "0.04530327703954148\n",
      "0.09577946805438842\n",
      "0.14210456137421024\n",
      "0.08295570407556495\n",
      "0.05194989273271252\n",
      "0.08249722288487538\n",
      "0.10559383403536533\n",
      "0.054213937196034866\n",
      "0.03925971964703682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5j/5y70dmpj6gz7vm0lv4mwhnzc0000gn/T/ipykernel_72717/1166497462.py:60: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(out, data_target_5[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007417113729296259\n",
      "0.005405717370318663\n",
      "0.006344360849933861\n",
      "0.0080123187969314\n",
      "0.01021104100210997\n",
      "0.017214229889616815\n",
      "0.015110647270885237\n",
      "0.014701260815768039\n",
      "0.013951082152167373\n",
      "0.014597822877143472\n",
      "0.010829679291083655\n",
      "0.006851640257862281\n",
      "0.009418932775366326\n",
      "0.010515561763292242\n",
      "0.007121505392959599\n",
      "0.007960224361068607\n",
      "0.005855639401689169\n",
      "0.00866901620539701\n",
      "0.006762948665770523\n",
      "0.01006064090165698\n",
      "0.00975679912128531\n",
      "0.013926937378961222\n",
      "0.014094190265916616\n",
      "0.021818999756895566\n",
      "0.012986535436453842\n",
      "0.015986279323847805\n",
      "0.019568216795543582\n",
      "0.011826880799834336\n",
      "0.016191781157222838\n",
      "0.007349493876286358\n",
      "0.0061793114689010055\n",
      "0.006386881693637716\n",
      "0.004592391883634213\n",
      "0.008596451853633157\n",
      "0.008563582526234573\n",
      "0.005465551411072868\n",
      "0.009037866264831235\n",
      "0.00812561215781703\n",
      "0.014002703655378375\n",
      "0.01102785108241668\n",
      "0.012955696486959434\n",
      "0.014122898003101259\n",
      "0.0171219660182747\n",
      "0.01396851835915673\n",
      "0.007033049454880711\n",
      "0.0052045562191787515\n",
      "0.009923527675336255\n",
      "0.010813701853382684\n",
      "0.017266247882219306\n",
      "0.009462453208273668\n",
      "0.010346500848455033\n",
      "0.009125807210247099\n",
      "0.005660315417774525\n",
      "0.009067603469907794\n",
      "0.014414178909533542\n",
      "0.014293550093768934\n",
      "0.010657486216402308\n",
      "0.006630115944912164\n",
      "0.008381985086634163\n",
      "0.004411993751168504\n",
      "0.009628151206769284\n",
      "0.009037055980455498\n",
      "0.009350015126681552\n",
      "0.01174400117639347\n",
      "0.009937667245965948\n",
      "0.010797086954309927\n",
      "0.009001243327771411\n",
      "0.008941634539865219\n",
      "0.009170777219476674\n",
      "0.008526127310389516\n",
      "0.007021512706932559\n",
      "0.005259246605274599\n",
      "0.009313498066066392\n",
      "0.004704128832466239\n",
      "0.007172869855203966\n",
      "0.00805632051332496\n",
      "0.009013965841446167\n",
      "0.013417873947438161\n",
      "0.007676525774076026\n",
      "0.008236843072076365\n",
      "0.0075596777130983485\n",
      "0.006410567897691602\n",
      "0.005470512766237796\n",
      "0.006894669649863739\n",
      "0.0072269799544421535\n",
      "0.0059276040497656625\n",
      "0.010133278494651645\n",
      "0.010440143285661391\n",
      "0.011844224228453056\n",
      "0.01038806866741685\n",
      "0.007084601299256649\n",
      "0.009894906149223946\n",
      "0.005884181763084635\n",
      "0.008031501607304432\n",
      "0.005081416384595312\n",
      "0.006372024970694627\n",
      "0.003293684818353516\n",
      "0.005978369107264164\n",
      "0.008238251570385298\n",
      "0.004139928328946699\n",
      "0.00807117683752263\n",
      "0.01181701533667365\n",
      "0.010716168532170636\n",
      "0.008799520064239914\n",
      "0.005019975206210866\n",
      "0.007803899716678708\n",
      "0.00423548081127592\n",
      "0.007429307531401719\n",
      "0.005527174625398895\n",
      "0.007858937160127531\n",
      "0.010905851881350554\n",
      "0.0052142676029149826\n",
      "0.006861177607248773\n",
      "0.008650873238258815\n",
      "0.006004601573531383\n",
      "0.009986068288819589\n",
      "0.006102041929226516\n",
      "0.007143613008437545\n",
      "0.008615527572798263\n",
      "0.010847728207641662\n",
      "0.01588746934458056\n",
      "0.008235152108303855\n",
      "0.011848153761422639\n",
      "0.008866980233230756\n",
      "0.005733155729412829\n",
      "0.00677331799483909\n",
      "0.0037118356573815196\n",
      "0.0042248532002551135\n",
      "0.0034449413969765276\n",
      "0.006021239789618278\n",
      "0.007336228939618325\n",
      "0.004577420525965538\n",
      "0.007162289264129689\n",
      "0.007642202030418892\n",
      "0.009367507192854535\n",
      "0.010570434599336608\n",
      "0.007561705629129197\n",
      "0.013719881840061317\n",
      "0.00942938522590144\n",
      "0.009960122031643712\n",
      "0.004860410642699711\n",
      "0.0044914043070789535\n",
      "0.005815196914702853\n",
      "0.0030130424039305327\n",
      "0.0053219614387487926\n",
      "0.006540051971568712\n",
      "0.004330083121849957\n",
      "0.009588869250190807\n",
      "0.008261891234420997\n",
      "0.008752945020068442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5j/5y70dmpj6gz7vm0lv4mwhnzc0000gn/T/ipykernel_72717/1166497462.py:73: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(out, data_target_6[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01075230260886755\n",
      "0.013073930336993066\n",
      "0.010892582339201045\n",
      "0.010735210966813914\n",
      "0.005403996236836447\n",
      "0.0045344507611260925\n",
      "0.006919906757063039\n",
      "0.011068410658537161\n",
      "0.016492359955136957\n",
      "0.01890139372094275\n",
      "0.015306911751173189\n",
      "0.033896293022529195\n",
      "0.02564551890241183\n",
      "0.016906869040368436\n",
      "0.013032116253087998\n",
      "0.01375836322658357\n",
      "0.014426960124046624\n",
      "0.007669122712918619\n",
      "0.00811703405943978\n",
      "0.007006172702680051\n",
      "0.00782915985461381\n",
      "0.011922331941270552\n",
      "0.017226754263535097\n",
      "0.009629062120094126\n",
      "0.016692708665307178\n",
      "0.00931416008670484\n",
      "0.006385551468664661\n",
      "0.003925824500578513\n",
      "0.006368217643743424\n",
      "0.00610828525477701\n",
      "0.004518063418662639\n",
      "0.004498860866726985\n",
      "0.017158108607128054\n",
      "0.018781971994481093\n",
      "0.0119585512806481\n",
      "0.01747529432958801\n",
      "0.019002197801457114\n",
      "0.015195981633201806\n",
      "0.01247883994157394\n",
      "0.014380607930943012\n",
      "0.009210903237431185\n",
      "0.005166851306096999\n",
      "0.008470932417897179\n",
      "0.009655524344053505\n",
      "0.014513330377452565\n",
      "0.00926132596786448\n",
      "0.004118557521114848\n",
      "0.008561810985087041\n",
      "0.01061910830169878\n",
      "0.00624508940301544\n",
      "0.011272682727826467\n",
      "0.008216194373621536\n",
      "0.009867946422715107\n",
      "0.006980830261912535\n",
      "0.012390795020823553\n",
      "0.010176331396761669\n",
      "0.009297316016611917\n",
      "0.003990965049185412\n",
      "0.005929959174702669\n",
      "0.006578495247275303\n",
      "0.006454798884337363\n",
      "0.008572057582350856\n",
      "0.013297701828670742\n",
      "0.007133090732815672\n",
      "0.008091921504320459\n",
      "0.006727792954542418\n",
      "0.012975447959172602\n",
      "0.010679153253176574\n",
      "0.008705711995355388\n",
      "0.005547928386467768\n",
      "0.009477501794277296\n",
      "0.006528229183242269\n",
      "0.006025049897271577\n",
      "0.005277330252705593\n",
      "0.010892371610209069\n",
      "0.009139227488680252\n",
      "0.007542448760928217\n",
      "0.007234665862283542\n",
      "0.011270087044262328\n",
      "0.0068159566914290405\n",
      "0.0077723220877353024\n",
      "0.007842633476127896\n",
      "0.00906485190124149\n",
      "0.0027825369532215828\n",
      "0.00309363942567458\n",
      "0.00531514437153295\n",
      "0.00680883101144562\n",
      "0.003376455794992117\n",
      "0.006030262277915789\n",
      "0.011294810232030272\n",
      "0.012218335985052921\n",
      "0.0076382266258824974\n",
      "0.01656758317669956\n",
      "0.009227043003165151\n",
      "0.011029036982142557\n",
      "0.004595845368241579\n",
      "0.004616816382615343\n",
      "0.004008412672074805\n",
      "0.003273610166673917\n",
      "0.006344175265829908\n",
      "0.003017875696808828\n",
      "0.003909476509324733\n",
      "0.005724535582087731\n",
      "0.007199950912725416\n",
      "0.006708505359470848\n",
      "0.00937020595762812\n",
      "0.012630409767268702\n",
      "0.007481279948586848\n",
      "0.006194328620572496\n",
      "0.009252155605430475\n",
      "0.005858208847417052\n",
      "0.003373952681257562\n",
      "0.004621706376667446\n",
      "0.009144201910160356\n",
      "0.00842462720373956\n",
      "0.007772586347594294\n",
      "0.007981849861624861\n",
      "0.005360267662284179\n",
      "0.0041263328848233135\n",
      "0.0077527092115029315\n",
      "0.006353224094681513\n",
      "0.005663529103525511\n",
      "0.00617849823290053\n",
      "0.0063554394977867815\n",
      "0.0021209115046524813\n",
      "0.003089212024740842\n",
      "0.005056273793765685\n",
      "0.0033649456489786163\n",
      "0.007536867249121464\n",
      "0.01564521621426086\n",
      "0.010663175662724823\n",
      "0.007208788515798902\n",
      "0.006636072808058414\n",
      "0.008116025398540812\n",
      "0.004961204346626502\n",
      "0.0051068898762073765\n",
      "0.0032353892404611622\n",
      "0.0017218854677172784\n",
      "0.002613568781810202\n",
      "0.0045293670435383755\n",
      "0.002987047480163741\n",
      "0.0033685613738658467\n",
      "0.007625030794199316\n",
      "0.006003988185813334\n",
      "0.007264146222980315\n",
      "0.017305206184217513\n",
      "0.012388621280656846\n",
      "0.008018788589544298\n",
      "0.009777844899950186\n",
      "0.01166110229055006\n"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "\n",
    "loss_avg = 0\n",
    "model_1.train()\n",
    "for epoch in range(epochs):\n",
    "    for idx, data in enumerate(data_line_1):\n",
    "        optimizer_1.zero_grad()\n",
    "        out = model_1(data)\n",
    "        loss = F.mse_loss(out, data_target_1[idx])\n",
    "        loss_avg += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_1.step()\n",
    "    print(loss_avg /((idx+1)))\n",
    "    loss_avg = 0\n",
    "\n",
    "model_2.train()\n",
    "for epoch in range(epochs):\n",
    "    for idx, data in enumerate(data_line_2):\n",
    "        optimizer_2.zero_grad()\n",
    "        out = model_2(data)\n",
    "        loss = F.mse_loss(out, data_target_2[idx])\n",
    "        loss_avg += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_2.step()\n",
    "    print(loss_avg /((idx+1)))\n",
    "    loss_avg = 0\n",
    "\n",
    "loss_avg = 0\n",
    "model_3.train()\n",
    "for epoch in range(epochs):\n",
    "    for idx, data in enumerate(data_line_3):\n",
    "        optimizer_3.zero_grad()\n",
    "        out = model_3(data)\n",
    "        loss = F.mse_loss(out, data_target_3[idx])\n",
    "        loss_avg += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_3.step()\n",
    "    print(loss_avg /((idx+1)))\n",
    "    loss_avg = 0\n",
    "\n",
    "loss_avg = 0\n",
    "model_4.train()\n",
    "for epoch in range(epochs):\n",
    "    for idx, data in enumerate(data_line_4):\n",
    "        optimizer_4.zero_grad()\n",
    "        out = model_4(data)\n",
    "        loss = F.mse_loss(out, data_target_4[idx])\n",
    "        loss_avg += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_4.step()\n",
    "    print(loss_avg /((idx+1)))\n",
    "    loss_avg = 0\n",
    "\n",
    "loss_avg = 0\n",
    "model_5.train()\n",
    "for epoch in range(epochs):\n",
    "    for idx, data in enumerate(data_line_5):\n",
    "        optimizer_5.zero_grad()\n",
    "        out = model_5(data)\n",
    "        loss = F.mse_loss(out, data_target_5[idx])\n",
    "        loss_avg += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_5.step()\n",
    "    print(loss_avg /((idx+1)))\n",
    "    loss_avg = 0\n",
    "\n",
    "loss_avg = 0\n",
    "model_6.train()\n",
    "for epoch in range(epochs):\n",
    "    for idx, data in enumerate(data_line_6):\n",
    "        optimizer_6.zero_grad()\n",
    "        out = model_6(data)\n",
    "        loss = F.mse_loss(out, data_target_6[idx])\n",
    "        loss_avg += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_6.step()\n",
    "    print(loss_avg /((idx+1)))\n",
    "    loss_avg = 0\n",
    "\n",
    "model_list = []\n",
    "model_list.append(model_1)\n",
    "model_list.append(model_2)\n",
    "model_list.append(model_3)\n",
    "model_list.append(model_4)\n",
    "model_list.append(model_5)\n",
    "model_list.append(model_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ddaa38-bd6e-47d2-82d3-c000f188886a",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8fd3f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_line(line):\n",
    "  if line == 'T010305': return 0\n",
    "  elif line == 'T010306': return 1\n",
    "  elif line == 'T050304': return 2\n",
    "  elif line == 'T050307': return 3\n",
    "  elif line == 'T100304': return 4\n",
    "  else : return 5\n",
    "\n",
    "def test_data_converter(data, data_index, data_mean, data_std):\n",
    "  head = 0\n",
    "  rtn = []\n",
    "  max = len(data_index)\n",
    "  for num, val in enumerate(data.index):\n",
    "    if data_index[head] == val:\n",
    "      if data[num] == 'NaN' :\n",
    "        rtn.append(data_mean[head])\n",
    "      else :\n",
    "        rtn.append((data[num] - data_mean[head]) / data_std[head])\n",
    "      head+=1\n",
    "    if head == max : break\n",
    "  return rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bdb9b20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 -0.7595997452735901\n",
      "36 2073292963840.0\n",
      "62 2073292963840.0\n",
      "63 2073292963840.0\n",
      "130 2073292963840.0\n",
      "131 2073292963840.0\n",
      "132 2086087294976.0\n",
      "248 2083289563136.0\n",
      "249 2073292963840.0\n",
      "250 2073292963840.0\n",
      "251 2073292963840.0\n",
      "252 2073292963840.0\n",
      "253 2073292963840.0\n",
      "254 2063295971328.0\n",
      "255 2073292963840.0\n",
      "260 2073292963840.0\n",
      "263 2073292963840.0\n",
      "280 2073292963840.0\n",
      "281 2073292963840.0\n",
      "282 2063295971328.0\n",
      "283 2073292963840.0\n",
      "284 2073292963840.0\n",
      "285 2073292963840.0\n",
      "286 2073292963840.0\n",
      "292 2073292963840.0\n",
      "293 2073292963840.0\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "result = []\n",
    "wrong_line = [0,0,0,0,0,0]\n",
    "\n",
    "for idx, data in test_df.iterrows():\n",
    "  i = find_line(test_line[idx])\n",
    "  test_data = test_data_converter(data, data_index[i], data_mean[i], data_std[i])\n",
    "  test_data = [float(x) for x in test_data]\n",
    "  output = model_list[i](torch.tensor(test_data))\n",
    "  result.append(output.item())\n",
    "  if output.item() >= 6.5 or output.item() <= -4.5 :\n",
    "    wrong_line[i] += 1\n",
    "  if i == 3: print(idx, output.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "722804b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 0, 25, 3, 0]\n",
      "[-0.24529075622558594, 0.2937888503074646, -0.13265138864517212, -1.197275996208191, -0.22859308123588562, -0.24852415919303894, -0.43976110219955444, -0.8156716227531433, -0.9417210817337036, 0.06436361372470856, -0.8166876435279846, -0.406818151473999, 1.4192980527877808, -0.7595997452735901, -0.3930935263633728, -0.21885454654693604, 0.19708271324634552, -0.020948752760887146, 0.6928340792655945, -0.47084784507751465, -0.3892507255077362, 0.5218356847763062, -0.13624799251556396, -0.7355822920799255, -0.5559015274047852, -0.08867907524108887, -0.04259173572063446, -0.3974859118461609, -0.6656548976898193, -0.37215542793273926, -1.1893064975738525, 0.03620123490691185, -0.29115843772888184, -0.15781351923942566, 0.33195918798446655, -1.1033382415771484, 2073292963840.0, -0.12399306893348694, 13953003520.0, -0.351319283246994, 13953003520.0, 0.33079126477241516, -0.48910099267959595, 0.31785547733306885, -0.5554553866386414, 0.07130832970142365, -0.18180786073207855, -2.5386734008789062, 0.41525447368621826, -2.360841751098633, 0.41614872217178345, -2.852597236633301, -1.6280266046524048, -1.7383370399475098, -2.1138927936553955, -1.8187763690948486, -0.3549762964248657, -0.6830135583877563, -1.9439523220062256, -1.561723232269287, -1.70945405960083, -0.6641009449958801, 2073292963840.0, 2073292963840.0, -1.466105341911316, -1.0720783472061157, -1.1844627857208252, 0.11940617859363556, 0.33305320143699646, 0.017450392246246338, -0.42254823446273804, -1.0947346687316895, 0.1931018829345703, -0.430700421333313, -0.11464345455169678, -0.20811206102371216, -0.11781547963619232, 0.7111318707466125, 0.07149693369865417, -11.279296875, 0.26066485047340393, 0.08431606739759445, 0.28134575486183167, 0.09582841396331787, 0.34501439332962036, 0.5839415192604065, -1.9728240966796875, -0.36326512694358826, -1.1587296724319458, -0.8585509657859802, 0.3853910565376282, 0.2110379934310913, 0.07100331783294678, 0.5515339970588684, -0.22503510117530823, 0.7057267427444458, -0.08371585607528687, 0.3803800344467163, 0.024378709495067596, 0.4177757501602173, -0.1428861916065216, 0.1920355260372162, 0.3788006901741028, 0.4668220281600952, 0.052650146186351776, 0.2470959573984146, 0.23015978932380676, -0.3151783347129822, 0.8015416860580444, -0.27640196681022644, -0.17068013548851013, -0.1806589961051941, 0.10611432790756226, 0.9943333268165588, 0.6213632822036743, 0.7572052478790283, -0.6207855343818665, -0.18260090053081512, 0.2373163402080536, 0.8557533025741577, -0.025934554636478424, 0.0767483115196228, 0.7274876832962036, 0.5404016971588135, 0.056997545063495636, -0.15359634160995483, 0.8321631550788879, 0.012770242989063263, -1.055317759513855, -0.3217678964138031, 2073292963840.0, 2073292963840.0, 2086087294976.0, 0.33598870038986206, 0.3149387538433075, 0.19394412636756897, -0.5236538052558899, 0.06160171329975128, 0.2233104109764099, -0.0850810706615448, 0.12266838550567627, -0.9720137715339661, -0.8594252467155457, -0.16869115829467773, 0.4756091833114624, 0.48226869106292725, -0.22890156507492065, 0.6166830658912659, -0.27711355686187744, 0.0018541663885116577, -0.5998916625976562, 0.4471033811569214, -0.054409682750701904, 0.09123775362968445, -0.2994648218154907, 0.43049800395965576, -0.35624703764915466, 0.16774660348892212, 0.6450950503349304, -0.1473976969718933, -0.07873103022575378, 1.3043885231018066, -0.09736345708370209, 0.6903722882270813, -0.2974777817726135, 0.1463536024093628, 0.12428951263427734, -0.3802107274532318, -0.3088536560535431, 0.9625419974327087, 0.9586451649665833, -0.19968563318252563, 1.0718677043914795, 0.1326790302991867, 0.7271857261657715, -0.2571011781692505, 0.7235251069068909, 0.9690890908241272, -0.30932533740997314, 1.0473337173461914, 0.9292810559272766, 0.05864278972148895, 0.12595784664154053, 1.3895070552825928, 0.07786606252193451, 0.2551923096179962, -0.4560861587524414, -7.218504905700684, -0.6359426379203796, -7.006477355957031, -0.6988955736160278, -7.450006484985352, -0.5067426562309265, 0.7175644636154175, 0.016378015279769897, 0.8675075173377991, 0.7067528963088989, -0.20905834436416626, 0.8312113881111145, -0.1511998176574707, 0.6761796474456787, -0.02359001338481903, 0.47247761487960815, -0.1608671247959137, 0.6262310743331909, 1.1227693557739258, 0.3521030843257904, 0.4748111963272095, -0.10069271922111511, 0.7277750968933105, -0.13679946959018707, -0.03560914844274521, 0.34892481565475464, -0.10256941616535187, 1.1478002071380615, 0.05341210216283798, 0.1890261024236679, 0.48440152406692505, 0.09889903664588928, 0.5651348233222961, 0.23140819370746613, 0.7326861619949341, 0.6432501673698425, 0.2217017412185669, 0.02177269756793976, -0.04254085570573807, 0.713451087474823, 0.00873567909002304, 0.2885141372680664, -0.14990738034248352, 0.32483428716659546, 0.026723921298980713, 0.19177016615867615, 0.7916475534439087, 0.6234972476959229, 0.139529287815094, -0.11751118302345276, -0.032229915261268616, 0.5338497757911682, -0.38323697447776794, -0.3392028212547302, 0.5454704165458679, -0.21728402376174927, 0.5980871319770813, 0.38982516527175903, 0.34568580985069275, 0.426941454410553, -0.061561256647109985, 2083289563136.0, 2073292963840.0, 2073292963840.0, 2073292963840.0, 2073292963840.0, 2073292963840.0, 2063295971328.0, 2073292963840.0, -0.15940570831298828, 0.4920397400856018, -0.12273640185594559, 0.23652702569961548, 2073292963840.0, 0.09879095107316971, -0.006033778190612793, 2073292963840.0, 0.9122838377952576, -0.31855204701423645, 0.32910555601119995, -0.05479976534843445, 0.04573865234851837, 0.26534655690193176, 0.0330788791179657, 0.33865249156951904, 0.4886586666107178, -0.07803863286972046, 0.05137857794761658, -0.16344307363033295, 0.04224718362092972, 0.17562556266784668, -0.039546579122543335, -0.34902700781822205, 2073292963840.0, 2073292963840.0, 2063295971328.0, 2073292963840.0, 2073292963840.0, 2073292963840.0, 2073292963840.0, -0.5315154790878296, -0.09029369056224823, 0.2392987608909607, 0.11160290241241455, 0.20859159529209137, 2073292963840.0, 2073292963840.0, -0.8221819400787354, 0.07879309356212616, 0.06364059448242188, -0.535226583480835, -0.3294242024421692, -0.14176756143569946, 0.262004017829895, 0.0073426254093647, -0.40471726655960083, -0.23350296914577484, -0.26313990354537964, 0.07757027447223663, 0.40224674344062805, 0.07721544802188873, 0.042461641132831573, 0.03314018249511719]\n"
     ]
    }
   ],
   "source": [
    "print (wrong_line)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c76612ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 227, 50]\n"
     ]
    }
   ],
   "source": [
    "result_final = []\n",
    "result_cnt = [0,0,0]\n",
    "for x in result:\n",
    "  if x <= -0.7875 : \n",
    "    result_cnt[0] += 1\n",
    "    result_final.append(0) \n",
    "  elif x >= 0.540 : \n",
    "    result_cnt[2] += 1\n",
    "    result_final.append(2)\n",
    "  else : \n",
    "    result_cnt[1] += 1\n",
    "    result_final.append(1)\n",
    "\n",
    "print(result_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97cf38e-2062-4645-9095-2ebac375711e",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b00cd-5680-4bf2-b7f8-0bea6ae8299e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['Y_Class'] = result_final\n",
    "submit.to_csv('./idea1_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2747a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cc4d55d52d1114a871817dc9d2b4e4d90b6d108a695844ce0d6cbea98b4fea22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
